---
title: "project3-test"
author: "Hanbo JIAO"
date: "2020/3/10"

---


```{r, eval=F}

packages.used=c("gbm", "tidyverse","caret")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))

# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
```


```{r}
library(caret) 
library(gbm)
library(tidyverse)
load("../output/fiducial_pt_list.RData")
```


```{r}
############为了之后的load/readxxx，提前设置file path###########

train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
info <- read.csv(train_label_path)
```




```{r exp_setup}
############设置一些参数，例如是否cross-validation, K-fold的参数，是否做各种train/test测试 ###########
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

```{r}
n<-length(fiducial_pt_list)

# #测试 partial part
# n<-n*0.3
# index<-sample(1:length(fiducial_pt_list),n,replace=F)
# n_train <- round(n*(4/5), 0)
# train_idx <- sample(index, n_train, replace = F)
# test_idx <- setdiff(index,train_idx)
set.seed(0306)
n_train <- round(n*(4/5), 0)
train_idx <- sample(1:n, n_train, replace = F)
test_idx <- setdiff(1:n,train_idx)


```

```{r feature}
#############这里feature函数直接可以求出78个点与其他77个点的距离。###########
source("../lib/feature.R")
# data_unsplit <- feature(fiducial_pt_list, 1:2500)
# save(data_unsplit, file="../output/feature_unsplit.RData")

############dat_train直接得出train_idx这些图片中，所有两点间距离（78*77=6006个）###########
# tm_feature_train <- NA
# if(run.feature.train){
#   tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
# }
############data_test同理###########
# tm_feature_test <- NA
# if(run.feature.test){
#   tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
# }


# save(dat_train, file="../output/feature_train.RData")
# save(dat_test, file="../output/feature_test.RData")


load("../output/feature_unsplit.RData")
dat_train<-data_unsplit[train_idx,]
dat_test<-data_unsplit[test_idx,]

# write.csv(data_unsplit, "../data/train_set/data_unsplit.csv")
# write.csv(dat_train, "../data/train_set/dat_train.csv")
# write.csv(dat_test, "../data/train_set/dat_test.csv")

```


```{r}
# data=dat_train#[sample(1:2000,500,replace = F),]





# distribution="adaboost"    #some problem
distribution = "multinomial"

data=data_unsplit#[sample(1:2500,1500,replace = F),]
train_idx <- sample(1:2500, 2000, replace = F)
test_idx <- setdiff(1:2500,train_idx)
data_train=data[train_idx,]
data_test=data[test_idx,]

n.tree=100
shrinkage=c(0.015)
cv.folds=10


```


```{r}
# data=data_unsplit[sample(1:2500,500,replace = F),]
# train_idx <- sample(1:500, 400, replace = F)
# test_idx <- setdiff(1:500,train_idx)
# data_train=data[train_idx,]
# data_test=data[test_idx,]
# n.tree=c(30)
# shrinkage=c(0.1)
# inter.dep =1
# cv.folds=c(2)


for (i in n.tree){
  for (j in shrinkage){
    for (k in cv.folds){
      
      n1<-Sys.time()
# fit initial model
gbm1 <-
  gbm(emotion_idx ~.,            # formula
    data=data_train,                   # dataset
    # var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
    #                              # +1: monotone increase,
    #                              #  0: no monotone restrictions
    distribution = distribution,   # see the help for other choices
    n.trees=500,              # number of trees
    shrinkage=0.05,         # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=1, # 1: additive model, 2: two-way interactions, etc.
    # bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    # train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 3,                # do 3-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=FALSE,               # don't print out progress
    n.cores=8)                   # use only a single core (detecting #cores is
                                 # error-prone, so avoided here)
n2<-Sys.time()-n1

#save(gbm1,file="../output/baseline.RData")


# 
best.iter_cv <- gbm.perf(gbm1,method="cv")


# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale (logit,log,etc.)


newdata<-data_test

pred = predict.gbm(object = gbm1,
                   newdata = newdata,
                   n.trees = best.iter_cv,
                   type = "response")

labels = colnames(pred)[apply(pred, 1, which.max)]
acc<-mean(newdata$emotion_idx==labels)
# table<-rbind(table,tibble(iter=i,acc=acc))


print(c(as.numeric(n2),i,j,k,best.iter_cv,acc))

    }
  }
}


# cm = confusionMatrix(test$Species, as.factor(labels))
# print(cm)

```



```{r}
point_data<-fiducial_pt_list%>%unlist%>%matrix(ncol=78*2,byrow = T)%>%as_tibble
x_train<-fiducial_pt_list[train_idx]
x_train<-x_train%>%unlist%>%matrix(ncol=78*2,byrow = T)%>%as_tibble

x_test<-fiducial_pt_list[test_idx]
x_test<-x_test%>%unlist%>%matrix(ncol=78*2,byrow = T)%>%as_tibble

y_train<-info$emotion_idx[train_idx]
y_test<-info$emotion_idx[test_idx]

train<-cbind(y_train,x_train)
test<-cbind(y_test,x_test)

gbm1 <-
  gbm( y_train~.,            # formula
    data=train,                   # dataset
    distribution = "multinomial",   # see the help for other choices
    shrinkage=0.01,         # shrinkage or learning rate,
    interaction.depth=3, # 1: additive model, 2: two-way interactions, etc.
    n.minobsinnode = 10,         # minimum total weight needed in each node
    n.trees = 200,
    cv.folds = 10,                # do 3-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=FALSE,               # don't print out progress
    n.cores=8)                   # use only a single core (detecting #cores is
                                 # error-prone, so avoided here)

# 
best.iter_cv <- gbm.perf(gbm1,method="cv")


# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale (logit,log,etc.)

newdata<-test

pred = predict.gbm(object = gbm1,
                   newdata = newdata,
                   n.trees = best.iter_cv,
                   type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
# least squares error

# result = data.frame(newdata$emotion_idx, labels)
acc<-mean(newdata$y_test==as.numeric(labels))

```


