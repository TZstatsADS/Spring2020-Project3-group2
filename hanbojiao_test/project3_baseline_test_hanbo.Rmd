---
title: "project3-test"
author: "Hanbo JIAO"
date: "2020/3/10"

---

## Step 1: Install the GBM library.
```{r, eval=F}
packages.used=c("gbm", "tidyverse")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))

# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
```

## Step 2: fit gradiant boosting models with no interactions (stumps)
```{r}
library(gbm)
library(tidyverse)
```


```{r}

n.tree
shrinkage
inter.dep
cv.folds
# fit initial model
gbm1 <-
gbm(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data,                   # dataset
    # var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
    #                              # +1: monotone increase,
    #                              #  0: no monotone restrictions
    distribution="adaboost",     # see the help for other choices
    n.trees=n.tree,              # number of trees
    shrinkage=shrinkage,         # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=inter.dep, # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = cv.folds,         # do 3-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=FALSE,               # don't print out progress
    n.cores=1)                   # use only a single core (detecting #cores is
                                 # error-prone, so avoided here)


# # check performance using an out-of-bag estimator
# # OOB underestimates the optimal number of iterations
# best.iter <- gbm.perf(gbm1,method="OOB")
# print(best.iter)
# 
# # check performance using a 50% heldout test set
# best.iter <- gbm.perf(gbm1,method="test")
# print(best.iter)

# check performance using 5-fold cross-validation
best.iter <- gbm.perf(gbm1,method="cv")
print(best.iter)

# plot the performance # plot variable influence
summary(gbm1,n.trees=1)         # based on the first tree
summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees


# make some new data


# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale (logit,log,etc.)
f.predict <- predict(gbm1,data2,best.iter)
# least squares error
print(sum((data2$Y-f.predict)^2))



```

