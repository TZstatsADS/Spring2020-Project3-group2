load("../output/fiducial_pt_list.RData")
############为了之后的load/readxxx，提前设置file path###########
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
info <- read.csv(train_label_path)
############设置一些参数，例如是否cross-validation, K-fold的参数，是否做各种train/test测试 ###########
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
n<-length(fiducial_pt_list)
# #测试 partial part
# n<-n*0.3
# index<-sample(1:length(fiducial_pt_list),n,replace=F)
# n_train <- round(n*(4/5), 0)
# train_idx <- sample(index, n_train, replace = F)
# test_idx <- setdiff(index,train_idx)
set.seed(0306)
n_train <- round(n*(4/5), 0)
train_idx <- sample(1:n, n_train, replace = F)
test_idx <- setdiff(1:n,train_idx)
#############这里feature函数直接可以求出78个点与其他77个点的距离。###########
source("../lib/feature.R")
# data_unsplit <- feature(fiducial_pt_list, 1:2500)
# save(data_unsplit, file="../output/feature_unsplit.RData")
############dat_train直接得出train_idx这些图片中，所有两点间距离（78*77=6006个）###########
# tm_feature_train <- NA
# if(run.feature.train){
#   tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
# }
############data_test同理###########
# tm_feature_test <- NA
# if(run.feature.test){
#   tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
# }
# save(dat_train, file="../output/feature_train.RData")
# save(dat_test, file="../output/feature_test.RData")
load("../output/feature_unsplit.RData")
dat_train<-data_unsplit[train_idx,]
dat_test<-data_unsplit[test_idx,]
n.tree=500
shrinkage=c(0.05,0.01)
inter.dep =1
cv.folds=c(2,3,5)
# distribution="adaboost"    #some problem
distribution = "multinomial"
data=data_unsplit#[sample(1:2500,1500,replace = F),]
train_idx <- sample(1:2500, 2000, replace = F)
test_idx <- setdiff(1:2500,train_idx)
data_train=data[train_idx,]
data_test=data[test_idx,]
for (i in n.tree){
for (j in shrinkage){
for (k in cv.folds){
n1<-Sys.time()
# fit initial model
gbm1 <-
gbm(emotion_idx ~.,            # formula
data=data_train,                   # dataset
# var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
#                              # +1: monotone increase,
#                              #  0: no monotone restrictions
distribution = distribution,   # see the help for other choices
n.trees=i,              # number of trees
shrinkage=j,         # shrinkage or learning rate,
# 0.001 to 0.1 usually work
interaction.depth=inter.dep, # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,        # fraction of data for training,
# first train.fraction*N used for training
n.minobsinnode = 10,         # minimum total weight needed in each node
cv.folds = k,                # do 3-fold cross-validation
keep.data=TRUE,              # keep a copy of the dataset with the object
verbose=FALSE,               # don't print out progress
n.cores=8)                   # use only a single core (detecting #cores is
# error-prone, so avoided here)
n2<-Sys.time()-n1
#save(gbm1,file="../output/baseline.RData")
# # check performance using a 50% heldout test set
best.iter_test <- gbm.perf(gbm1,method="test")
# # check performance using 5-fold cross-validation
best.iter_cv <- gbm.perf(gbm1,method="cv")
# predict on the new data using "best" number of trees
# f.predict generally will be on the canonical scale (logit,log,etc.)
best.iter<-c(best.iter_test,best.iter_cv)
newdata<-data_test
table<-NULL
acc<-rep(NULL,2)
for (l in 1:2){
pred = predict.gbm(object = gbm1,
newdata = newdata,
n.trees = best.iter[l],
type = "response")
labels = colnames(pred)[apply(pred, 1, which.max)]
# least squares error
# result = data.frame(newdata$emotion_idx, labels)
acc[l]<-mean(newdata$emotion_idx==labels)
# table<-rbind(table,tibble(iter=i,acc=acc))
}
print(c(as.numeric(n2),i,j,k,best.iter[1],acc[1],best.iter[2],acc[2]))
}
}
}
library(caret)
library(gbm)
library(tidyverse)
load("../output/fiducial_pt_list.RData")
############为了之后的load/readxxx，提前设置file path###########
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
info <- read.csv(train_label_path)
############设置一些参数，例如是否cross-validation, K-fold的参数，是否做各种train/test测试 ###########
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
n<-length(fiducial_pt_list)
# #测试 partial part
# n<-n*0.3
# index<-sample(1:length(fiducial_pt_list),n,replace=F)
# n_train <- round(n*(4/5), 0)
# train_idx <- sample(index, n_train, replace = F)
# test_idx <- setdiff(index,train_idx)
set.seed(0306)
n_train <- round(n*(4/5), 0)
train_idx <- sample(1:n, n_train, replace = F)
test_idx <- setdiff(1:n,train_idx)
#############这里feature函数直接可以求出78个点与其他77个点的距离。###########
source("../lib/feature.R")
# data_unsplit <- feature(fiducial_pt_list, 1:2500)
# save(data_unsplit, file="../output/feature_unsplit.RData")
############dat_train直接得出train_idx这些图片中，所有两点间距离（78*77=6006个）###########
# tm_feature_train <- NA
# if(run.feature.train){
#   tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
# }
############data_test同理###########
# tm_feature_test <- NA
# if(run.feature.test){
#   tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
# }
# save(dat_train, file="../output/feature_train.RData")
# save(dat_test, file="../output/feature_test.RData")
load("../output/feature_unsplit.RData")
dat_train<-data_unsplit[train_idx,]
dat_test<-data_unsplit[test_idx,]
library(caret)
library(gbm)
library(tidyverse)
load("../output/fiducial_pt_list.RData")
############为了之后的load/readxxx，提前设置file path###########
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
info <- read.csv(train_label_path)
############设置一些参数，例如是否cross-validation, K-fold的参数，是否做各种train/test测试 ###########
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
n<-length(fiducial_pt_list)
# #测试 partial part
# n<-n*0.3
# index<-sample(1:length(fiducial_pt_list),n,replace=F)
# n_train <- round(n*(4/5), 0)
# train_idx <- sample(index, n_train, replace = F)
# test_idx <- setdiff(index,train_idx)
set.seed(0306)
n_train <- round(n*(4/5), 0)
train_idx <- sample(1:n, n_train, replace = F)
test_idx <- setdiff(1:n,train_idx)
#############这里feature函数直接可以求出78个点与其他77个点的距离。###########
source("../lib/feature.R")
# data_unsplit <- feature(fiducial_pt_list, 1:2500)
# save(data_unsplit, file="../output/feature_unsplit.RData")
############dat_train直接得出train_idx这些图片中，所有两点间距离（78*77=6006个）###########
# tm_feature_train <- NA
# if(run.feature.train){
#   tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
# }
############data_test同理###########
# tm_feature_test <- NA
# if(run.feature.test){
#   tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
# }
# save(dat_train, file="../output/feature_train.RData")
# save(dat_test, file="../output/feature_test.RData")
load("../output/feature_unsplit.RData")
dat_train<-data_unsplit[train_idx,]
dat_test<-data_unsplit[test_idx,]
data(HouseVotes84, package = "mlbench")
install.packages("mlbench")
data(HouseVotes84, package = "mlbench")
HouseVotes84
names(HouseVotes84)
dat_train%>%names
model <- naiveBayes(emotion_idx ~ ., data = dat_train, laplace = 3)
label<-predict(model, dat_test%>%select(-emotion_idx), type = "raw")
table(label, dat_test$emotion_idx)
mean(label== dat_test$emotion_idx)
model <- naiveBayes(emotion_idx ~ ., data = dat_train, laplace = 3)
library(e1071)
model <- naiveBayes(emotion_idx ~ ., data = dat_train, laplace = 3)
label<-predict(model, dat_test%>%select(-emotion_idx), type = "raw")
table(label, dat_test$emotion_idx)
mean(label== dat_test$emotion_idx)
label
label<-predict(model, dat_test%>%select(-emotion_idx))
table(label, dat_test$emotion_idx)
mean(label== dat_test$emotion_idx)
tabel(label)
table(label)
View(fiducial_pt_list)
typeof
typeof(fiducial_pt_list)
typeof(dat_test)
model <- naiveBayes(emotion_idx ~ ., data = dat_train, laplace = 2)
label<-predict(model, dat_test%>%select(-emotion_idx))
table(label, dat_test$emotion_idx)
mean(label== dat_test$emotion_idx)
mean(label== dat_test$emotion_idx)
xx<-predict(model, dat_train%>%select(-emotion_idx))
mean(xx== dat_train$emotion_idx)
fiducial_pt_list
dim(fiducial_pt_list)
length(fiducial_pt_list)
names(fiducial_pt_list)
View(fiducial_pt_list)
fiducial_pt_list[[1]]
fiducial_pt_list[1]
fiducial_pt_list[c(2,3)]
fiducial_pt_list[train_idx]%>%lengfth
fiducial_pt_list[train_idx]%>%length
info$emotion_idx
info$emotion_idx[train_idx]%>%length
x_train<-data_unsplit[train_idx,]
x_test<-data_unsplit[test_idx,]
y_train<-info$emotion_idx[train_idx]%>%length
y_test<-info$emotion_idx[test_idx]%>%length
x_train<-data_unsplit[train_idx,]
x_test<-data_unsplit[test_idx,]
y_train<-info$emotion_idx[train_idx]%>%length
y_test<-info$emotion_idx[test_idx]%>%length
x_train
x_train<-fiducial_pt_list[train_idx]
y_traint<-fiducial_pt_list[test_idx]
y_train<-info$emotion_idx[train_idx]
y_test<-info$emotion_idx[test_idx]
y_test
x_test<-fiducial_pt_list[test_idx]
x_test
gbm1 <-
gbm(x_train ~.y_train,            # formula
# data=data_train,                   # dataset
# var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
#                              # +1: monotone increase,
#                              #  0: no monotone restrictions
distribution = distribution,   # see the help for other choices
n.trees=i,              # number of trees
shrinkage=j,         # shrinkage or learning rate,
# 0.001 to 0.1 usually work
interaction.depth=inter.dep, # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
train.fraction = 0.5,        # fraction of data for training,
# first train.fraction*N used for training
n.minobsinnode = 10,         # minimum total weight needed in each node
cv.folds = k,                # do 3-fold cross-validation
keep.data=TRUE,              # keep a copy of the dataset with the object
verbose=FALSE,               # don't print out progress
n.cores=8)                   # use only a single core (detecting #cores is
library(caret)
library(gbm)
library(tidyverse)
load("../output/fiducial_pt_list.RData")
############为了之后的load/readxxx，提前设置file path###########
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")
info <- read.csv(train_label_path)
############设置一些参数，例如是否cross-validation, K-fold的参数，是否做各种train/test测试 ###########
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
n<-length(fiducial_pt_list)
# #测试 partial part
# n<-n*0.3
# index<-sample(1:length(fiducial_pt_list),n,replace=F)
# n_train <- round(n*(4/5), 0)
# train_idx <- sample(index, n_train, replace = F)
# test_idx <- setdiff(index,train_idx)
set.seed(0306)
n_train <- round(n*(4/5), 0)
train_idx <- sample(1:n, n_train, replace = F)
test_idx <- setdiff(1:n,train_idx)
write.csv(dat_train, "../dat_train.csv")
load("../output/feature_unsplit.RData")
dat_train<-data_unsplit[train_idx,]
dat_test<-data_unsplit[test_idx,]
write.csv(dat_test, file="../dat_test.csv")
write.csv(dat_train, "../dat_train.csv")
write.csv(data_unsplit, "../data/train_set/data_unsplit")
write.csv(data_unsplit, "../data/train_set/data_unsplit.csv")
packages.used=c("gbm", "tidyverse","caret","reticulate")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(reticulate)
y
conda activate r-reticulate
conda install pandas
library(reticulate)
scipy <- import("scipy")
use_python("/usr/local/bin/python")
library(reticulate)
Sys.which("python")
use_virtualenv("myenv")
use_virtualenv("myenv")
py_install("pandas")
library(reticulate)
use_virtualenv("myenv")
py_install("pandas")
py_install("numpy")
py_install("sklearn")
py_install("sklearn")
conda_install("sklearn")
py_install(pip = TRUE)
py_install(pip = TRUE)
# library(reticulate)
# use_virtualenv("myenv")
# py_install("pandas")
# py_install("numpy")
# py_install("sklearn")
# py_install("xgboost")
# py_install("matplotlib")
py_install(pip = TRUE)
# py_install("xgboost")
# py_install("matplotlib")
py_install(pip = TRUE)
# library(reticulate)
# use_virtualenv("myenv")
# py_install("pandas")
# py_install("numpy")
# py_install("xgboost")
# py_install("matplotlib")
py_install(pip = TRUE)
from sklearn.model_selection import train_test_split
reticulate::py_config()
library(reticulate)
sklearn <- import("sklearn")
py_config()
library(reticulate)
# create a new environment
conda_create("r-reticulate")
# install SciPy
conda_install("r-reticulate", "scipy")
# import SciPy (it will be automatically discovered in "r-reticulate")
scipy <- import("scipy")
library(reticulate)
# create a new environment
conda_create("r-reticulate")
# install SciPy
conda_install("r-reticulate", "sklearn")
py_discover_config()
knitr::opts_chunk$set(collapse = TRUE)
# install.packages("reticulate")
library(reticulate)
py_discover_config()
import sys
knitr::opts_chunk$set(collapse = TRUE)
# install.packages("reticulate")
library(reticulate)
py_discover_config()
knitr::opts_chunk$set(collapse = TRUE, engine.path = list(python = 'E:/R-3.6.2/library/reticulate/python'))
conda_install("sklearn")
knitr::opts_chunk$set(collapse = TRUE, engine.path = list(python = 'E:/R-3.6.2'))
conda_install("sklearn")
conda_install("sklearn" pip = T)
conda_install("sklearn" ,pip = T)
conda_install("sklearn" ,pip = T,python_version = 3.6)
library(reticulate)
# create a new environment
conda_create("r-reticulate")
# install SciPy
conda_install("r-reticulate", "scipy")
py_install("pandas")
library(reticulate)
py_install("pandas")
py_install("pandas", envname = "r-reticulate", method = c("auto",
"virtualenv", "conda"), conda = "auto")
)
py_install("pandas", envname = "r-reticulate", method = c("auto","virtualenv", "conda"), conda = "auto")
py_install("pandas", envname = "r-reticulate", method = "auto", conda = "auto")
py_install("pandas",  method = "auto", conda = "auto")
use_condaenv("myenv")
use_condaenv("myenv")
use_virtualenv("~/myenv")
py_install("pandas",  method = "auto", conda = "auto")
conda update --all
py_discover_config()
py_config()
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
fiducial_pt_list
fiducial_pt_list[[1:2]]
fiducial_pt_list[[1]]
fiducial_pt_list[[c(1:3)]]
fiducial_pt_list%>%head(3)
fiducial_pt_list%>%head(3)->x
index=1:3
input_list<-x
### here is an example of extracting pairwise distances between fiducial points
### Step 1: Write a function pairwise_dist to calculate pairwise distance of items in a vector
pairwise_dist <- function(vec){
### input: a vector(length n), output: a vector containing pairwise distances(length n(n-1)/2)
return(as.vector(dist(vec)))
}
### Step 2: Write a function pairwise_dist_result to apply function in Step 1 to column of a matrix
pairwise_dist_result <-function(mat){
### input: a n*2 matrix(e.g. fiducial_pt_list[[1]]), output: a vector(length n(n-1))
return(as.vector(apply(mat, 2, pairwise_dist)))
}
### Step 3: Apply function in Step 2 to selected index of input list, output: a feature matrix with ncol = n(n-1) = 78*77 = 6006
pairwise_dist_feature <- t(sapply(input_list[index], pairwise_dist_result))
dim(pairwise_dist_feature)
input_list[index]
index[[1]]
x[[1]]
x[[1]]->y
mat=y
apply(mat, 2, pairwise_dist)
dim(apply(mat, 2, pairwise_dist))
apply(mat, 2, pairwise_dist)
as.vector(apply(mat, 2, pairwise_dist))
dim(as.vector(apply(mat, 2, pairwise_dist)))
as.vector(apply(mat, 2, pairwise_dist))%>%length
apply(mat, 2, pairwise_dist))
apply(mat, 2, pairwise_dist)
dim(apply(mat, 2, pairwise_dist))
yy<-apply(mat, 2, pairwise_dist)
View(yy)
mat
apply(mat, 2, pairwise_dist)
dist()
dist(apply(mat, 2, pairwise_dist))
mat
mat[1]
1
mat[1:3,]
mat[1:5,]
mat[1:5,]->mat
apply(mat, 2, pairwise_dist)
as.vector(apply(mat, 2, pairwise_dist))
mat
mat[1:2,]
mat[1:2,]->mat
apply(mat, 2, pairwise_dist)
apply(mat, 1, pairwise_dist)
?dist()
x <- c(0, 0, 1, 1, 1, 1)
y <- c(1, 0, 1, 1, 0, 1)
data=rbind(x, y)
data
dist(rbind(x, y))
?apply
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
apply(x, 2, mean, trim = .2)
x
apply(x, 2, pairwise_dist)
x
x[1:2,]
x[1:2,]->x
apply(mat, 2, pairwise_dist)
apply(x, 2, pairwise_dist)
x
dist(x)
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
dist(x)
x
dist(x)
mat
dist(mat)
input_list
sapply(input_list[index], mat)
sapply(input_list[index], dist)
input_list
pairwise_dist_feature <- t(sapply(input_list[index], dist))
dim(pairwise_dist_feature)
source("../lib/feature_euclidean.R")
data_unsplit_euclidean <- feature_euclidean(fiducial_pt_list, 1:2500)
data_unsplit_euclidean
write.csv(data_unsplit_euclidean, "../data/train_set/data_unsplit_euclidean.csv")
detect_core()
detectCores()
detectCores
library(parallel)
detectCores()
